Defining Software
A textbook description of software might take the following form:   Software is: (1) instructions (computer programs) that when executed provide desired   features, function, and performance; (2) data structures that enable the programs to adequately manipulate information, and (3) descriptive information in both hard copy and   virtual forms that describes the operation and use of the programs. But a more formal definition probably won’t measurably improve your understanding. Software is developed or engineered; it is not manufactured in the classical sense. In both   activities, high quality is achieved through good design, but the manufacturing phase for hardware can introduce quality problems that are nonexistent          (or easily corrected) for software. Both activities are dependent on people,   but the relationship between people applied and work accomplished is   entirely different (see Chapter 24). Software doesn’t “wear out.”   Figure 1.1 depicts failure rate as a function of time for hardware. The relationship, often called the “bathtub curve,” indicates that hardware exhibits   relatively high failure rates early in its life (these failures are often attributable to design or manufacturing defects); defects are corrected and the failure   rate drops to a steady-state level (hopefully, quite low) for some period of   time. In theory, therefore, the failure rate curve for software   should take the form of the “idealized curve” shown in Figure 1.2. However, these are corrected and the curve flattens as shown. However, the implication is clear—software doesn’t wear out.        This seeming contradiction can best be explained by considering the   actual curve in Figure 1.2. During its life,2 software will undergo change. As   changes are made, it is likely that errors will be introduced, causing the   failure rate curve to spike as shown in the “actual curve” (Figure 1.2). Before   the curve can return to the original steady-state failure rate, another change   is requested, causing the curve to spike again. Slowly, the minimum failure   rate level begins to rise—the software is deteriorating due to change. Another aspect of wear illustrates the difference between hardware and   software. Every software failure indicates an   error in design or in the process through which design was translated into   machine executable code. Therefore, the software maintenance tasks that   accommodate requests for change involve considerably more complexity   than hardware maintenance. Although the industry is moving toward component-based construction, most   software continues to be custom built. As an engineering discipline evolves, a collection of standard design components is created. In the hardware world, component reuse is a natural part of   the engineering process. In the software world, it is something that has only   begun to be achieved on a broad scale. Modern reusable components encapsulate both data and the processing that is applied to the data, enabling the   software engineer to create new applications from reusable parts.3 For example, today’s interactive user interfaces are built with reusable components   that enable the creation of graphics windows, pull-down menus, and a wide   variety of interaction mechanisms. The data structures and processing detail   required to build the interface are contained within a library of reusable   components for interface construction.
 
Software Application Domains
In either case, the systems software area is characterized by heavy interaction   with computer hardware; heavy usage by multiple users; concurrent operation that requires scheduling, resource sharing, and sophisticated process   management; complex data structures; and multiple external interfaces. Application software—stand-alone programs that solve a specific business   need. Applications in this area process business or technical data in a way   that facilitates business operations or management/technical decision making. In addition to conventional data processing applications, application   software is used to control business functions in real time (e.g., point-of-sale   transaction processing, real-time manufacturing process control). Engineering/scientific software—has been characterized by “number   crunching” algorithms. Applications range from astronomy to volcanology,   from automotive stress analysis to space shuttle orbital dynamics, and   from molecular biology to automated manufacturing. Computer-aided design, system simulation, and other interactive applications have begun to take on real-time and   even system software characteristics. Embedded software—resides within a product or system and is used to   implement and control features and functions for the end user and for the   system itself. Embedded software can perform limited and esoteric functions   (e.g., key pad control for a microwave oven) or provide significant function   and control capability (e.g., digital functions in an automobile such as fuel   control, dashboard displays, and braking systems). Product-line software can focus on a limited and   esoteric marketplace (e.g., inventory control products) or address mass   consumer markets (e.g., word processing, spreadsheets, computer graphics,   multimedia, entertainment, database management, and personal and   business financial applications). In their simplest form, WebApps can   be little more than a set of linked hypertext files that present information   using text and limited graphics. Applications within this area include robotics, expert systems,   pattern recognition (image and voice), artificial neural networks, theorem   proving, and game playing. In some cases, new systems are being built, but in   many others, existing applications are being corrected, adapted, and enhanced. Hopefully, the legacy to be left behind by this generation will   ease the burden of future software engineers. And yet, new challenges (Chapter 31)   have appeared on the horizon:   Open-world computing—the rapid growth of wireless networking may   soon lead to true pervasive, distributed computing.        Netsourcing—the World Wide Web is rapidly becoming a computing engine   as well as a content provider. Open source—a growing trend that results in distribution of source code for   systems applications (e.g., operating systems, database, and development environments) so that many people can contribute to its development.
 
SOFTWARE ENGINEERING
It follows that a concerted effort should be made to understand the   problem before a software solution is developed. • The information technology requirements demanded by individuals, businesses, and governments grow increasing complex with each passing year. Large teams of people now create computer programs that were once built   by a single individual. It   follows that design becomes a pivotal activity. If the software fails, people and major enterprises can experience   anything from minor inconvenience to catastrophic failures. It follows that   software should exhibit high quality. • As the perceived value of a specific application grows, the likelihood is that   its user base and longevity will also grow. As its user base and time-in-use          increase, demands for adaptation and enhancement will also grow. You will be tempted to add to this definition.9 It says little about the technical aspects of software quality; it does not directly address the need for customer satisfaction or timely product delivery; it omits mention of the importance of measurement   and metrics; it does not state the importance of an effective process. And yet, Bauer’s   definition provides us with a baseline. We need discipline, but we also need   adaptability and agility. Referring to Figure 1.3, any engineering approach (including software engineering) must rest on an organizational commitment to quality. The bedrock that supports software engineering is a quality focus. The foundation for software engineering is the process layer. Process defines a framework          that must be established for effective delivery of software engineering technology. The software process forms the basis for management control of software projects   and establishes the context in which technical methods are applied, work products   (models, documents, data, reports, forms, etc.) are produced, milestones are established, quality is ensured, and change is properly managed. Software engineering methods provide the technical how-to’s for building software. Methods encompass a broad array of tasks that include communication,   requirements analysis, design modeling, program construction, testing, and support. Software engineering tools provide automated or semiautomated support for the   process and the methods. When tools are integrated so that information created by   one tool can be used by another, a system for the support of software development,   called computer-aided software engineering, is established.
 
THE SOFTWARE PROCESS
An activity strives to achieve a broad objective   (e.g., communication with stakeholders) and is applied regardless of the application   domain, size of the project, complexity of the effort, or degree of rigor with which   software engineering is to be applied. An action (e.g., architectural design) encompasses a set of tasks that produce a major work product (e.g., an architectural design   model). A task focuses on a small, but well-defined objective (e.g., conducting a unit   test) that produces a tangible outcome. In the context of software engineering, a process is not a rigid prescription for how   to build computer software. Rather, it is an adaptable approach that enables the people doing the work (the software team) to pick and choose the appropriate set of   work actions and tasks. The intent is always to deliver software in a timely manner   and with sufficient quality to satisfy those who have sponsored its creation and those   who will use it.        A process framework establishes the foundation for a complete software engineering process by identifying a small number of framework activities that are applicable to all software projects, regardless of their size or complexity. Any complicated journey can be simplified if a map exists. A   software project is a complicated journey, and the planning activity creates a   “map” that helps guide the team as it makes the journey. The map—called a   software project plan—defines the software engineering work by describing   the technical tasks to be conducted, the risks that are likely, the resources   that will be required, the work products to be produced, and a work   schedule. Whether you’re a landscaper, a bridge builder, an aeronautical   engineer, a carpenter, or an architect, you work with models every day. You   create a “sketch” of the thing so that you’ll understand the big picture—what   it will look like architecturally, how the constituent parts fit together, and   many other characteristics. If required, you refine the sketch into greater and   greater detail in an effort to better understand the problem and how you’re   going to solve it. This activity combines code generation (either manual or   automated) and the testing that is required to uncover errors in the code. The software (as a complete entity or as a partially completed increment) is delivered to the customer who evaluates the delivered   product and provides feedback based on the evaluation. That is, communication, planning, modeling, construction,   and deployment are applied repeatedly through a number of project iterations. In general, umbrella activities are applied throughout a software project and help a software team manage and control progress, quality,   change, and risk. Typical umbrella activities include:   Software project tracking and control—allows the software team to   assess progress against the project plan and take any necessary action to   maintain the schedule. Software quality assurance—defines and conducts the activities required   to ensure software quality. Therefore, a process adopted for one project might be significantly   different than a process adopted for another project. Among the differences are   • Overall flow of activities, actions, and tasks and the interdependencies   among them   • Degree to which actions and tasks are defined within each framework activity   • Degree to which work products are identified and required          • Manner in which quality assurance activities are applied   • Manner in which project tracking and control activities are applied   • Overall degree of detail and rigor with which the process is described   • Degree to which the customer and other stakeholders are involved with the   project   • Level of autonomy given to the software team   • Degree to which team organization and roles are prescribed   In Part 1 of this book, I’ll examine software process in considerable detail. Prescriptive   process models (Chapter 2) stress detailed definition, identification, and application   of process activities and tasks. Their intent is to improve system quality, make projects more manageable, make delivery dates and costs more predictable, and guide   teams of software engineers as they perform the work required to build a system. If   prescriptive models are applied dogmatically and without adaptation, they can increase the level of bureaucracy associated with building computer-based systems   and inadvertently create difficulty for all stakeholders. Agile process models (Chapter 3) emphasize project “agility” and follow a set of principles that lead to a more informal (but, proponents argue, no less effective) approach   to software process. These process models are generally characterized as “agile” because they emphasize maneuverability and adaptability.
 
A GENERIC PROCESS MODEL
A GENERIC PROCESS MODEL   In Chapter 1, a process was defined as a collection of work activities, actions, and   tasks that are performed when some work product is to be created. Each software engineering action is defined by a task set that identifies the work   tasks that are to be completed, the work products that will be produced, the quality   assurance points that will be required, and the milestones that will be used to indicate progress. As I discussed in Chapter 1, a generic process framework for software engineering defines five framework activities—communication, planning, modeling,   construction, and deployment. In addition, a set of umbrella activities—project   tracking and control, risk management, quality assurance, configuration management, technical reviews, and others—are applied throughout the process. This aspect—called process flow—describes how the framework activities and the actions and tasks that occur within each framework   activity are organized with respect to sequence and time and is illustrated in   Figure 2.2. A parallel process flow (Figure 2.2d) executes one or   more activities in parallel with other activities (e.g., modeling for one aspect of the   software might be executed in parallel with construction of another aspect of the   software).        2.1.1
 
Agile Modeling (AM) 
Simply put, Agile Modeling (AM) is a collection of values,   principles, and practices for modeling software that can be applied on a software development project in an effective and light-weight manner. The agile modeling philosophy recognizes that an agile team must have the courage   to make decisions that may cause it to reject a design and refactor. Although AM suggests a wide array of “core” and “supplementary” modeling principles, those that make AM unique are [Amb02a]:   Model with a purpose. A developer who uses AM should have a specific   goal (e.g., to communicate information to the customer or to help better understand some aspect of the software) in mind before creating the model. Once the goal for the model is identified, the type of notation to be used and   level of detail required will be more obvious. AM suggests that to provide needed insight, each model should   present a different aspect of the system and only those models that provide   value to their intended audience should be used. Travel light. As software engineering work proceeds, keep only those models that will provide long-term value and jettison the rest. Every work product   that is kept must be maintained as changes occur. This represents work that   slows the team down. Ambler [Amb02a] notes that “Every time you decide to   keep a model you trade-off agility for the convenience of having that information available to your team in an abstract manner (hence potentially enhancing communication within your team as well as with project stakeholders).”   Content is more important than representation. Modeling should impart information to its intended audience. The modeling approach should be adapted to the needs of   the agile team. A major segment of the software engineering community has adopted the Unified   Modeling Language (UML)16 as the preferred method for representing analysis and   design models.
 
REQUIREMENTS ENGINEERING
REQUIREMENTS ENGINEERING   Designing and building computer software is challenging, creative, and just plain   fun. In fact, building software is so compelling that many software developers want   to jump right in before they have a clear understanding of what is needed. They argue   that things will become clear as they build, that project stakeholders will be able to   understand need only after examining early iterations of the software, that things   change so rapidly that any attempt to understand requirements in detail is a waste   of time, that the bottom line is producing a working program and all else is secondary. From a software process perspective,   requirements engineering is a major software engineering action that begins during   the communication activity and continues into the modeling activity. Requirements engineering builds a bridge to design and construction. But where   does the bridge originate? Others might suggest that it begins with a broader   system definition, where software is but one component of the larger system   domain. But regardless of the starting point, the journey across the bridge takes you          high above the project, allowing you to examine the context of the software work to   be performed; the specific needs that design and construction must address; the priorities that guide the order in which work is to be completed; and the information,   functions, and behaviors that will have a profound impact on the resultant design. Requirements engineering provides the appropriate mechanism for understanding what the customer wants, analyzing need, assessing feasibility, negotiating a reasonable solution, specifying the solution unambiguously, validating the specification,   and managing the requirements as they are transformed into an operational system   [Tha97]. It encompasses seven distinct tasks: inception, elicitation, elaboration,   negotiation, specification, validation, and management. How does a software project get started? Is there a single event that   becomes the catalyst for a new computer-based system or product, or does the need   evolve over time? In some cases,   a casual conversation is all that is needed to precipitate a major software engineering effort. But in general, most projects begin when a business need is identified   or a potential new market or service is discovered. Stakeholders from the business   community (e.g., business managers, marketing people, product managers) define   a business case for the idea, try to identify the breadth and depth of the market, do a   rough feasibility analysis, and identify a working description of the project’s scope. All of this information is subject to change, but it is sufficient to precipitate discussions with the software engineering organization.2   At project inception,3 you establish a basic understanding of the problem, the people who want a solution, the nature of the solution that is desired, and the effectiveness of preliminary communication and collaboration between the other stakeholders   and the software team. It certainly seems simple enough—ask the customer, the users, and   others what the objectives for the system or product are, what is to be accomplished,   how the system or product fits into the needs of the business, and finally, how the system or product is to be used on a day-to-day basis. But it isn’t simple—it’s very hard. • Problems of scope. The boundary of the system is ill-defined or the   customers/users specify unnecessary technical detail that may confuse,   rather than clarify, overall system objectives. The customers/users are not completely sure   of what is needed, have a poor understanding of the capabilities and limitations of their computing environment, don’t have a full understanding of the   problem domain, have trouble communicating needs to the system engineer,   omit information that is believed to be “obvious,” specify requirements that   conflict with the needs of other customers/users, or specify requirements   that are ambiguous or untestable. • Problems of volatility. To help overcome these problems, you must approach requirements gathering in an   organized manner. The information obtained from the customer during inception and   elicitation is expanded and refined during elaboration. Elaboration is driven by the creation and refinement of user scenarios that describe how the end user (and other actors) will interact with the system. Each user   scenario is parsed to extract analysis classes—business domain entities that are   visible to the end user. The attributes of each analysis class are defined, and the services4 that are required by each class are identified. Using an iterative approach that prioritizes requirements, assesses   their cost and risk, and addresses internal conflicts, requirements are eliminated,   combined, and/or modified so that each party achieves some measure of satisfaction. A specification can be a written document, a set of graphical models, a formal mathematical model, a collection   of usage scenarios, a prototype, or any combination of these. The work products produced as a consequence of requirements engineering are assessed for quality during a validation step. Requirements for computer-based systems   change, and the desire to change requirements persists throughout the life of the   system. Requirements management is a set of activities that help the project team   identify, control, and track requirements and changes to requirements at any time as   the project proceeds.6 Many of these activities are identical to the software configuration management (SCM) techniques discussed in Chapter 22.          5.2
 
SOFTWARE QUALITY
In the most general sense,   software quality can be defined1 as: An effective software process applied in a manner   that creates a useful product that provides measurable value for those who produce it   and those who use it. There is little question that the preceding definition could be modified or extended   and debated endlessly. The management aspects   of process create the checks and balances that help avoid project chaos—a   key contributor to poor quality. Software engineering practices allow the   developer to analyze the problem and design a solid solution—both critical   to building high-quality software. In addition, it satisfies a set of implicit   requirements (e.g., ease of use) that are expected of all high-quality software. By adding value for both the producer and user of a software product, highquality software provides benefits for the software organization and the enduser community. The software organization gains added value because   high-quality software requires less maintenance effort, fewer bug fixes, and   reduced customer support. This enables software engineers to spend more   time creating new applications and less on rework. The user community   gains added value because the application provides a useful capability in   a way that expedites some business process. The end result is (1) greater   software product revenue, (2) better profitability when an application   supports a business process, and/or (3) improved availability of information   that is crucial for the business.
 
THE ISO 9000 QUALITY STANDARDS
These systems cover a wide variety of activities encompassing a product’s entire life   cycle including planning, controlling, measuring, testing and reporting, and improving quality levels throughout the development and manufacturing process. Upon successful registration,   a company is issued a certificate from a registration body represented by the auditors. Semiannual surveillance audits ensure continued compliance to the standard.        The requirements delineated by ISO 9001:2000 address topics such as management responsibility, quality system, contract review, design control, document and   data control, product identification and traceability, process control, inspection and   testing, corrective and preventive action, control of quality records, internal quality   audits, training, servicing, and statistical techniques. If you desire   further information on ISO 9001:2000, see [Ant06], [Mut03], or [Dob04].
 
THE CMMI
Today, it has evolved into   the Capability Maturity Model Integration (CMMI) [CMM07], a comprehensive process   meta-model that is predicated on a set of system and software engineering capabilities that should be present as organizations reach different levels of process capability and maturity. The CMMI represents a process meta-model in two different ways: (1) as a   “continuous” model and (2) as a “staged” model. The continuous CMMI metamodel describes a process in two dimensions as illustrated in Figure 30.2. Each   process area (e.g., project planning or requirements management) is formally   assessed against specific goals and practices and is rated according to the following capability levels:   Level 0: Incomplete—the process area (e.g., requirements management) is   either not performed or does not achieve all goals and objectives defined by   the CMMI for level 1 capability for the process area. Work tasks required to produce defined   work products are being conducted. In addition, all work associated with the process area conforms to an organizationally   defined policy; all people doing the work have access to adequate resources   to get the job done; stakeholders are actively involved in the process area as   required; all work tasks and work products are “monitored, controlled, and   reviewed; and are evaluated for adherence to the process description” [CMM07]. In addition, the process is “tailored from the organization’s set of standard   processes according to the organization’s tailoring guidelines, and contributes work products, measures, and other process-improvement information to the organizational process assets” [CMM07]. In addition, the process area is controlled and improved using   measurement and quantitative assessment. “Quantitative objectives for quality and process performance are established and used as criteria in managing   the process” [CMM07]. In   addition, the process area is adapted and optimized using quantitative   (statistical) means to meet changing customer needs and to continually   improve the efficacy of the process area under consideration.        For example, project planning is one of eight process areas defined by the CMMI   for “project management” category.6 The specific goals (SG) and the associated specific practices (SP) defined for project planning are [CMM07]:   SG 1 Establish Estimates   SP 1.1-1 Estimate the Scope of the Project   SP 1.2-1 Establish Estimates of Work Product and Task Attributes   SP 1.3-1 Define Project Life Cycle   SP 1.4-1 Determine Estimates of Effort and Cost   SG 2 Develop a Project Plan   SP 2.1-1 Establish the Budget and Schedule   SP 2.2-1 Identify Project Risks   SP 2.3-1 Plan for Data Management   SP 2.4-1 Plan for Project Resources   SP 2.5-1 Plan for Needed Knowledge and Skills   SP 2.6-1 Plan Stakeholder Involvement   SP 2.7-1 Establish the Project Plan   SG 3 Obtain Commitment to the Plan   SP 3.1-1 Review Plans That Affect the Project   SP 3.2-1 Reconcile Work and Resource Levels   SP 3.3-1 Obtain Plan Commitment   In addition to specific goals and practices, the CMMI also defines a set of five   generic goals and related practices for each process area. Hence, to achieve a particular   capability level, the generic goal for that level and the generic practices that correspond to that goal must be achieved. The   relationship between maturity levels and process areas is shown in Figure 30.3.          30.4
 
WHITE-BOX TESTING
WHITE-BOX TESTING   White-box testing, sometimes called glass-box testing, is a test-case design philosophy that uses the control structure described as part of component-level design to   derive test cases.
 
THE RMMM PLAN
THE RMMM PLAN   A risk management strategy can be included in the software project plan, or the risk   management steps can be organized into a separate risk mitigation, monitoring, and   management plan (RMMM). The RMMM plan documents all work performed as part   of risk analysis and is used by the project manager as part of the overall project   plan. In most   cases, the RIS is maintained using a database system so that creation and information entry, priority ordering, searches, and other analysis may be accomplished easily. As I have already discussed, risk mitigation is a problem avoidance activity. Another job of   risk monitoring is to attempt to allocate origin [what risk(s) caused which problems   throughout the project].               28.8
 
SOFTWARE MAINTENANCE
They’ll need a few enhancements   to make it work in their world. The challenge of software maintenance has begun. You’re faced with a growing   queue of bug fixes, adaptation requests, and outright enhancements that must be          planned, scheduled, and ultimately accomplished. Before long, the queue has grown   long and the work it implies threatens to overwhelm the available resources. As time   passes, your organization finds that it’s spending more money and time maintaining   existing programs than it is engineering new applications. You may ask why so much maintenance is required and why so much effort is   expended. Even when   these programs were created using the best design and coding techniques known at the   time [and most were not], they were created when program size and storage space were   principle concerns. They were then migrated to new platforms, adjusted for changes in   machine and operating system technology and enhanced to meet new user needs—all   without enough regard to overall architecture. The result is the poorly designed structures, poor coding, poor logic, and poor documentation of the software systems we are   now called on to keep running . Another reason for the software maintenance problem is the mobility of software   people. Worse, other generations of software people have modified the   system and moved on. And today, there may be no one left who has any direct   knowledge of the legacy system. As I noted in Chapter 22, the ubiquitous nature of change underlies all software   work. Throughout this book, I’ve emphasized the importance of understanding the   problem (analysis) and developing a well-structured solution (design). In essence, maintainability is a qualitative indication1 of the ease with   which existing software can be corrected, adapted, or enhanced. It makes use of design patterns (Chapter 12) that allow ease of understanding. It has been constructed using well-defined coding standards and conventions, leading to source code that is self-documenting and understandable. Therefore, the design and implementation of the software must “assist” the person who is making the change.
 
